#!/usr/bin/env python3
import argparse
import json
import signal
import subprocess
import sys
from contextlib import ExitStack
from dataclasses import dataclass
from pathlib import Path
from urllib.parse import urlparse


# Credit: https://stackoverflow.com/a/1094933
def _sizeof_fmt(num: float, suffix: str = "B") -> str:
    for unit in ("", "Ki", "Mi", "Gi", "Ti", "Pi", "Ei", "Zi"):
        if abs(num) < 1024.0:
            return f"{num:3.1f} {unit}{suffix}"
        num /= 1024.0
    return f"{num:.1f} Yi{suffix}"


def _run_dockerized_ogr2ogr(database_url, directory: Path, path: Path, options: list):
    assert directory.is_absolute()
    assert path.is_absolute()

    # Make ogr2ogr access database tunnel connection open on the host
    # https://stackoverflow.com/a/43541732
    urlobj = urlparse(database_url)
    database_url_from_container = urlobj._replace(
        netloc=f"{urlobj.username}:{urlobj.password}@host.docker.internal:{urlobj.port}"
    ).geturl()

    command = [
        "docker",
        "compose",
        "-f",
        "docker-compose.yml",
        "--profile",
        "gdal",
        "run",
        "--rm",  # Do not leave dead containers
        "-v",
        f"{directory}:/home/data",
        "gdal",
        "ogr2ogr",
        # Setup connection to the PostgreSQL database
        # See specific PostgreSQL options here:
        # https://gdal.org/drivers/vector/pg.html
        "-f",
        "PostgreSQL",
        f"PG:{database_url_from_container}",
        *options,
        str(Path("/home/data") / path.relative_to(directory)),
    ]

    return subprocess.run(command)


@dataclass
class Table:
    name: str
    select_sql: str | list[str] | None = None

    def __post_init__(self):
        if isinstance(self.select_sql, list):
            self.select_sql = "\n".join(self.select_sql)


@dataclass
class Config:
    tables: list[Table]


def main(
    config: Config, directory: Path, url: str | None, overwrite: bool, yes: bool
) -> int:
    if not directory.exists():
        print(f"ERROR: directory {directory} does not exist", file=sys.stderr)
        return 1

    tables_by_name = {table.name: table for table in config.tables}
    geopackages = []

    for theme in ("TRANSPORT", "ADRESSES"):
        search_pattern = f"**/1_DONNEES_LIVRAISON_*/*/{theme}/*.gpkg"

        for path in directory.glob(search_pattern):
            if path.stem in tables_by_name:
                geopackages.append({"path": path, "table": tables_by_name[path.stem]})

    if not geopackages:
        print(
            f"ERROR: no matching geopackages found at {directory} ",
            file=sys.stderr,
        )
        return 1

    # Improve pg disk usage by importing smallest packages first.
    # Reason: ogr2ogr runs TRUNCATE *after* importing the 1st package.
    geopackages = sorted(geopackages, key=lambda t: t["path"].stat().st_size)

    with ExitStack() as exit_stack:
        if url is not None:
            urlobj = urlparse(url)

            if urlobj.scheme not in ("postgres", "postgresql"):
                print(
                    f"ERROR: invalid database URL: {url}: "
                    f"unexpected scheme: {urlobj.scheme} "
                    "(expected postgres:// or postgresql://)"
                )
                return 1

            if urlobj.hostname not in ("localhost", "127.0.0.1"):
                print(
                    "ERROR: expected url to be on localhost, "
                    f"received: {urlobj.hostname}"
                )
                return 1

            database_url = url
        else:
            app = "dialog-bdtopo"

            print(f"===> Opening DB tunnel to {app}...")

            tunnel_proc = exit_stack.enter_context(
                subprocess.Popen(
                    ["./tools/scalingodbtunnel", app, "--host-url"],
                    stdout=subprocess.PIPE,
                )
            )

            def _close_tunnel():
                print("---> Stopping tunnel...")
                tunnel_proc.send_signal(signal.SIGINT)
                tunnel_proc.wait()

            exit_stack.callback(_close_tunnel)

            database_url = (
                tunnel_proc.stdout.readline()
                .decode()
                .strip()  # Remove any final newline
            )

            if not database_url:
                print(
                    "ERROR: failed to open DB tunnel, see output above",
                    file=sys.stderr,
                )
                return 1

            print("---> Tunnel open.")

        # ogr2ogr only seems to support postgresql://, especially when query parameters
        # are provided, such as '?sslmode=prefer' by Scalingo.
        database_url = database_url.replace("postgres://", "postgresql://")

        print("===> Import info:")
        print("Database URL:", database_url)
        print("Import mode:", "overwrite" if overwrite else "append")

        if not yes and input("------> Proceed? (y/N) ") != "y":
            return 1

        print("===> Importing BD TOPO content...")
        tables_seen = set()

        try:
            for gpkg in geopackages:
                path = gpkg["path"]  # type: Path
                table = gpkg["table"]  # type: Table

                pretty_size = _sizeof_fmt(path.stat().st_size)
                print(f"------> Import into {table.name}: {path} ({pretty_size})")

                options = [
                    # See general ogr2ogr options here:
                    # https://gdal.org/programs/ogr2ogr.html
                    "-progress",
                    # Define the table name on initial import
                    "-nln",
                    table.name,
                    # Reproject all geometries to standard WGS 84 projection, aka EPSG 4326
                    # (Geopackages in BD TOPO may not all use the same projection)
                    "-t_srs",
                    "EPSG:4326",
                    # Append to existing data, or overwrite for faster re-ingestion
                    "-overwrite" if overwrite and table.name not in tables_seen else "-append",
                    # Use provided SQL statement, if any, to only import certain columns
                    # (In this case, progress bar won't be available due a limitation of the ogr2ogr Postgres driver)
                    *(["-sql", table.select_sql] if table.select_sql else []),
                    # Enable slightly faster ingestion
                    # https://gdal.org/drivers/vector/pg.html#config-PG_USE_COPY
                    "--config",
                    "PG_USE_COPY",
                    "YES",
                    # Explicitly set the name of the FID column to create
                    "-lco",
                    "FID=ogc_fid",
                ]

                if overwrite and table.name not in tables_seen:
                    options.extend(
                        [
                            # Remove existing tuples before importing (saves disk space
                            # at the cost of some unavailability during execution)
                            # https://gis.stackexchange.com/a/357040
                            # This should only run once per table so that all packages
                            # are effectively appended.
                            "--config",
                            "OGR_TRUNCATE",
                            "YES",
                        ]
                    )

                    tables_seen.add(table.name)

                result = _run_dockerized_ogr2ogr(
                    database_url,
                    directory=directory,
                    path=path,
                    options=options,
                )

                if result.returncode:
                    print(
                        "ERROR: ogr2ogr command failed, see output above",
                        file=sys.stderr,
                    )
                    return 1

            print("------> Import successful!")

            # Clear up dead tuples and update table statistics so that the
            # tables can be queried immediately.
            print("===> Running VACUUM ANALYZE...")
            analyze_stmt = f"VACUUM (ANALYZE, VERBOSE) {', '.join(tables_seen)};"
            print("------>", analyze_stmt)
            result = subprocess.run(
                [
                    "psql",
                    database_url,
                    # Avoid 'No space left on device' error when available RAM is short
                    "-c",
                    "SET max_parallel_maintenance_workers = 0;",
                    "-c",
                    analyze_stmt,
                ]
            )
            result.check_returncode()
            print("------> VACUUM ANALYZE ran successfully!")

            if overwrite:
                print(
                    "===> NOTE: run 'make bdtopo_migrate_redo' "
                    "to recreate indexes"
                )
        except KeyboardInterrupt:
            # ogr2ogr already manages most of the cleanup for us, nothing to do.
            return 2
        else:
            print("===> Done")

    return 0


def _path_check_exists(value: str) -> Path:
    path = Path(value)

    if not path.exists():
        raise argparse.ArgumentTypeError(f"file or directory not found: {value}")

    return path


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "directory",
        type=_path_check_exists,
        help="Path to directory containing BD TOPO data",
    )
    parser.add_argument(
        "--prod", action="store_true", help="Confirm deployment to 'dialog-bdtopo' app"
    )
    parser.add_argument(
        "--url",
        help="Deploy to a PostgreSQL database identified by this database URL",
        default=None,
    )
    parser.add_argument(
        "--overwrite", action="store_true", help="Recreate tables instead of appending"
    )
    parser.add_argument("-y", "--yes", action="store_true", help="Accept all prompts")
    parser.add_argument(
        "-c",
        "--config",
        type=_path_check_exists,
        default=Path(__file__).parent / "bdtopo_update.config.json",
        help="Path to config file. Default: ./bdtopo_update.config.json",
    )
    args = parser.parse_args()

    if args.url is None and not args.prod:
        print(
            "ERROR: please pass --prod to deploy to 'dialog-bdtopo' environment, "
            "or --url to target a specific PostgreSQL database",
            file=sys.stderr,
        )
        sys.exit(1)

    configdata = json.loads(args.config.read_text())

    config = Config(
        tables=[Table(**t) for t in configdata["tables"]],
    )

    # Relative paths are OK but ogr2ogr volume will require an absolute path.
    directory = args.directory.absolute()

    sys.exit(
        main(
            config,
            directory=directory,
            url=args.url,
            overwrite=args.overwrite,
            yes=args.yes,
        )
    )
